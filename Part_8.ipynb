{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Part 8",
      "metadata": {
        "tags": [],
        "cell_id": "00000-73fa56e6-5fed-4cbe-8e35-5e6e23d1b9b9",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Lecture 8\nUsed for:\n- Learn compression to store large datasets\n- pre-training for feature learning\n- density estimation\n- initializing weights\n- generating new data samples\n\n**Size of hidden layer**:\n- Undercomplete \n    - *h < x*\n    - Compress the input, good for training samples\n- Overcomplete \n    - *h > x*\n    - No compression, useful for representation learning\n    - copying input could be prevented with **regularization**\n\n<img src=\"./image/over_and_undercomplete.png\" height=\"300\" />\n\n- Denoising autoencoder\n    - Use **regularization** to make robust to noise\n    - $g(f(x+e)) = x$\n    - Corrupt the data on purpose \n- Contractive autoencoder\n    - Penalize unwanted variations\n        - if x changes, h does not change much; **robust**\n    - Frobenius norm of the Jacobian $\\omega(h)$ measures:\n        - **how much the activations change when input changes**.\n\n**Generate samples**?\n- Auto encoder does not allow for that; is not continuous\n- Variational allows since it has a hidden layer distribution sampling\n\n<img src=\"./image/Encoders.png\" height=\"200\" />\n\n**Generative Adversarial Networks**\n- **Goal**: \n    - get a discriminator output of > 0.5\n        - generate new sample images\n- _sample from high dimensional training distribution_\n    - add random noise, learn transformation etc.\n- networks:\n    - **Generator**: create real looking images\n        - Aims to minimize D, such that it is close to 1 (for fake data)\n        - **Only needs random noise as input**\n    - **Discriminator**: judge if real or fake\n        - Aims to maximize D, such that it is close to 1 (for real data)\n- Evaluating likelihoods: Higher likelihood for images does not necessarily mean visually better\n\n**VAE** vs **AE**:\n- Latent space of VAE has all points in latent space close to the origin, yielding meaningful reproductions\n    - Continuous space, which AE has **not**.\n- VAE penalizes the **structure of the latent space**\n- VAE and AE penalize **reconstructions**",
      "metadata": {
        "tags": [],
        "cell_id": "00001-256c8c7d-edf6-4e04-8681-9c5f8f9c0e98",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Assignment 8: (Variational) auto-encoders\nGenerating \"meaningful\" samples using unsupervised learning. Unsupervised learners could be used to exploit (hidden) useful structure in data.\n\n**Dimension reduction**: \n- reduce amount of features without losing (most important) information. \n    <!-- - $$g: \\mathbb{R}^n \\rightarrow \\mathbb{R}^k \\quad \\quad n \\gg k$$ -->\n    - Conversion of *bmp* to *jpeg*.\n\n<img src=\"./image/dimension_reduction.png\" height=\"250\" />\n\n**The Auto-encoder**:\n- The outcome ($y =: \\hat{x}$) is compared to the the input so it learns both how to encode the input signals and decode it back. \n- Is the concatanation of encoder and decoder: $$y = h(g(x)).$$\n- Loss: $$f_L = L \\left( x, y \\right) = L \\left( x, h(g(x)) \\right).$$\n- $L$ is mostly chosen to be the MSE-loss.\n- Assumes not all data is equally important, there is always some noise and it can be reduced to something that only contains the most important information.\n\n<img src=\"./image/auto-encoder.png\" height=\"250\" />\n\n** Latent space**:\n- the encoded vector lives in a low dimensional space. \n    - Often called: **Latent space**\n    - Potential of generating new data samples\n- Latent space gaps can cause the generated examples to be \"gibberish\", which happens when you only penalize the outcomes\n- Variational auto-encoders (VAE's) penalize also the structure of the latent space. This results in generated examples to be less \"gibberish\".\n\n<img src=\"./image/auto-encoder-architecture.png\" height=\"250\" />\n\n**Architecture**:\n- to 100 means, limit dimensions to 100 variables\n- Make use of MSE-loss because this is a regression and not classification problem.",
      "metadata": {
        "tags": [],
        "cell_id": "00001-a132ca15-6219-4f08-b10f-23d876c432ed",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00002-2a488ea1-6640-4752-8956-ce1512a61625",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "60bbf30d",
        "execution_millis": 22,
        "execution_start": 1618052641940,
        "deepnote_cell_type": "code"
      },
      "source": "import torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torchinfo import summary\n\n\n#encoder\nclass Encoder(nn.Module):\n    def __init__(self, latent_dims, s_img, hdim):\n        super(Encoder, self).__init__()\n        self.linear1 = nn.Linear(s_img*s_img, hdim[0])\n        self.linear2 = nn.Linear(hdim[0], hdim[1])\n        self.linear3 = nn.Linear(hdim[1], latent_dims)\n        self.relu    = nn.ReLU()\n\n    def forward(self, x):\n        x = torch.flatten(x, start_dim=1)\n        x = self.relu(self.linear1(x))\n        x = self.relu(self.linear2(x))\n        x = self.linear3(x)\n\n        return x\n\n#decoder\nclass Decoder(nn.Module):\n    def __init__(self, latent_dims, s_img, hdim):\n        super(Decoder, self).__init__()\n        self.linear1 = nn.Linear(latent_dims, hdim[1])\n        self.linear2 = nn.Linear(hdim[1], hdim[0])\n        self.linear3 = nn.Linear(hdim[0], s_img*s_img)\n        self.relu    = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, z):\n        z = self.relu(self.linear1(z))\n        z = self.relu(self.linear2(z))\n        z = self.sigmoid(self.linear3(z))\n        z = z.reshape((-1, 1, s_img, s_img))\n\n        return z\n\n#autoencoder\nclass Autoencoder(nn.Module):\n    def __init__(self, latent_dims, s_img, hdim = [100, 50]):\n        super(Autoencoder, self).__init__()\n        self.encoder = Encoder(latent_dims, s_img, hdim)\n        self.decoder = Decoder(latent_dims, s_img, hdim)\n\n    def forward(self, x):\n        z = self.encoder(x)\n        y = self.decoder(z)\n\n        return y\n\n\n# Learnable parameters: \nn_samples, in_channels, s_img, latent_dims = 3, 1, 28, 2\nhdim = [100, 50] #choose hidden dimension\nbias = False\n\nmodel_ouput = summary(\n    Autoencoder(latent_dims, s_img, hdim),\n    (n_samples, in_channels, s_img, s_img),\n    verbose=2,\n    col_width=16,\n    col_names=[\"input_size\", \"output_size\", \"kernel_size\", \"num_params\", \"mult_adds\"],\n)",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "========================================================================================================================\nLayer (type:depth-idx)                   Input Shape      Output Shape     Kernel Shape     Param #          Mult-Adds\n========================================================================================================================\n├─Encoder: 1-1                           [3, 1, 28, 28]   [3, 2]           --               --               --\n|    └─linear1.weight                                                      [100, 784]\n|    └─linear2.weight                                                      [50, 100]\n|    └─linear3.weight                                                      [2, 50]\n|    └─Linear: 2-1                       [3, 784]         [3, 100]         [784, 100]       78,500           235,200\n|    └─ReLU: 2-2                         [3, 100]         [3, 100]         --               --               --\n|    └─Linear: 2-3                       [3, 100]         [3, 50]          [100, 50]        5,050            15,000\n|    └─ReLU: 2-4                         [3, 50]          [3, 50]          --               --               --\n|    └─Linear: 2-5                       [3, 50]          [3, 2]           [50, 2]          102              300\n├─Decoder: 1-2                           [3, 2]           [3, 1, 28, 28]   --               --               --\n|    └─linear1.weight                                                      [50, 2]\n|    └─linear2.weight                                                      [100, 50]\n|    └─linear3.weight                                                      [784, 100]\n|    └─Linear: 2-6                       [3, 2]           [3, 50]          [2, 50]          150              300\n|    └─ReLU: 2-7                         [3, 50]          [3, 50]          --               --               --\n|    └─Linear: 2-8                       [3, 50]          [3, 100]         [50, 100]        5,100            15,000\n|    └─ReLU: 2-9                         [3, 100]         [3, 100]         --               --               --\n|    └─Linear: 2-10                      [3, 100]         [3, 784]         [100, 784]       79,184           235,200\n|    └─Sigmoid: 2-11                     [3, 784]         [3, 784]         --               --               --\n========================================================================================================================\nTotal params: 168,086\nTrainable params: 168,086\nNon-trainable params: 0\nTotal mult-adds (M): 1.25\n========================================================================================================================\nInput size (MB): 0.01\nForward/backward pass size (MB): 0.03\nParams size (MB): 0.67\nEstimated Total Size (MB): 0.71\n========================================================================================================================\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "**Variational Auto Encoders**:\n- Penalize structure and outcome of latent space.\n- **Reparameterization trick**:\n    - Used to allow backpropagation\n    - $$z = \\mu_x +  \\sigma_x \\zeta= g_1(x) + g_2 (x) \\zeta$$ where $\\zeta$ is randomly sampled from $\\mathcal{N} (0, I)$.\n- _Encoder_: Generates a distribution from which z is chosen randomly. \n    - Yields a **continuous** and **complete** latent space.\n    - This is because our encouder outputs a range of possible values from which we'll randomly sample to feed into the decorder model.\n- _Decoder_: generate new data\n-  the latent space is regularized if the distribution is penalized.\n    - the regularization term tries to make the network learn a normal distribution close to mean 0 and variance of 1. \n    - the reproduction term  is maximizing the reconstruction likelihood\n    - the regularization term is to encourage learned distribution to be similar to the true prior distribution which we assume to follow Gaussian distribution.\n\n$$\n\\begin{aligned}\nf_{L} &= \\underbrace{L (x, h(z))}_{\\text{reproduction term}} + \\underbrace{R \\left(\\mathcal{N} (\\mu_x, \\sigma_x), \\mathcal{N} (0, I) \\right)}_{\\text{regularization term}} \\\\\n&= L (x, h(z)) + R \\left(\\mathcal{N} (g_2 (x), g_1(x)), \\mathcal{N} (0, I) \\right)\n\\end{aligned}\n$$\n\n<img src=\"./image/var_encoder.png\" height=\"300\" />",
      "metadata": {
        "tags": [],
        "cell_id": "00002-9b3022bf-b923-4e96-8e06-7486511cdcdd",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00004-f8830535-ff61-4667-8615-df8aac67904c",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "6c209c0e",
        "execution_millis": 22,
        "execution_start": 1618052560956,
        "deepnote_cell_type": "code"
      },
      "source": "import torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torchinfo import summary\n\n\n#encoder\nclass VarEncoder(nn.Module):\n    def __init__(self, latent_dims, s_img, hdim):\n        super(VarEncoder, self).__init__()\n        \n        #layers for g1\n        self.linear1_1 = nn.Linear(s_img*s_img, hdim[0])\n        self.linear2_1 = nn.Linear(hdim[0], hdim[1])\n        self.linear3_1 = nn.Linear(hdim[1], latent_dims)\n\n        #layers for g2\n        self.linear1_2 = nn.Linear(s_img*s_img, hdim[0])\n        self.linear2_2 = nn.Linear(hdim[0], hdim[1])\n        self.linear3_2 = nn.Linear(hdim[1], latent_dims)\n\n        self.relu    = nn.ReLU()\n\n        #distribution setup\n        self.N = torch.distributions.Normal(0, 1)\n        self.N.loc = self.N.loc\n        self.N.scale = self.N.scale\n        self.kl = 0\n\n    def kull_leib(self, mu, sigma):\n        return (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()\n\n    def reparameterize(self, mu, sig):\n        return mu + sig*self.N.sample(mu.shape)\n\n    def forward(self, x):\n        x = torch.flatten(x, start_dim=1)\n        \n        x1 = self.relu(self.linear1_1(x))\n        x1 = self.relu(self.linear2_1(x1))\n\n        x2 = self.relu(self.linear1_2(x))\n        x2 = self.relu(self.linear2_2(x2))\n\n        sig = torch.exp(self.linear3_1(x1))\n        mu = self.linear3_2(x2)\n\n        #reparameterize to find z\n        z = self.reparameterize(mu, sig)\n\n        #loss between N(0,I) and learned distribution\n        self.kl = self.kull_leib(mu, sig)\n\n        return z\n\n#decoder\nclass Decoder(nn.Module):\n    def __init__(self, latent_dims, s_img, hdim):\n        super(Decoder, self).__init__()\n        self.linear1 = nn.Linear(latent_dims, hdim[1])\n        self.linear2 = nn.Linear(hdim[1], hdim[0])\n        self.linear3 = nn.Linear(hdim[0], s_img*s_img)\n        self.relu    = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, z):\n        z = self.relu(self.linear1(z))\n        z = self.relu(self.linear2(z))\n        z = self.sigmoid(self.linear3(z))\n        z = z.reshape((-1, 1, s_img, s_img))\n\n        return z\n\n#autoencoder\nclass VarAutoencoder(nn.Module):\n    def __init__(self, latent_dims, s_img, hdim = [100, 50]):\n        super(VarAutoencoder, self).__init__()\n        self.encoder = VarEncoder(latent_dims, s_img, hdim)\n        self.decoder = Decoder(latent_dims, s_img, hdim)\n\n    def forward(self, x):\n        z = self.encoder(x)\n        y = self.decoder(z)\n\n        return y\n\n\n# Learnable parameters: \nn_samples, in_channels, s_img, latent_dims = 3, 1, 28, 2\nhdim = [100, 50] #choose hidden dimension\nbias = False\n\nmodel_ouput = summary(\n    VarAutoencoder(latent_dims, s_img, hdim),\n    (n_samples, in_channels, s_img, s_img),\n    verbose=2,\n    col_width=16,\n    col_names=[\"input_size\", \"output_size\", \"kernel_size\", \"num_params\", \"mult_adds\"],\n)",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "========================================================================================================================\nLayer (type:depth-idx)                   Input Shape      Output Shape     Kernel Shape     Param #          Mult-Adds\n========================================================================================================================\n├─VarEncoder: 1-1                        [3, 1, 28, 28]   [3, 2]           --               --               --\n|    └─linear1_1.weight                                                    [100, 784]\n|    └─linear2_1.weight                                                    [50, 100]\n|    └─linear3_1.weight                                                    [2, 50]\n|    └─linear1_2.weight                                                    [100, 784]\n|    └─linear2_2.weight                                                    [50, 100]\n|    └─linear3_2.weight                                                    [2, 50]\n|    └─Linear: 2-1                       [3, 784]         [3, 100]         [784, 100]       78,500           235,200\n|    └─ReLU: 2-2                         [3, 100]         [3, 100]         --               --               --\n|    └─Linear: 2-3                       [3, 100]         [3, 50]          [100, 50]        5,050            15,000\n|    └─ReLU: 2-4                         [3, 50]          [3, 50]          --               --               --\n|    └─Linear: 2-5                       [3, 784]         [3, 100]         [784, 100]       78,500           235,200\n|    └─ReLU: 2-6                         [3, 100]         [3, 100]         --               --               --\n|    └─Linear: 2-7                       [3, 100]         [3, 50]          [100, 50]        5,050            15,000\n|    └─ReLU: 2-8                         [3, 50]          [3, 50]          --               --               --\n|    └─Linear: 2-9                       [3, 50]          [3, 2]           [50, 2]          102              300\n|    └─Linear: 2-10                      [3, 50]          [3, 2]           [50, 2]          102              300\n├─Decoder: 1-2                           [3, 2]           [3, 1, 28, 28]   --               --               --\n|    └─linear1.weight                                                      [50, 2]\n|    └─linear2.weight                                                      [100, 50]\n|    └─linear3.weight                                                      [784, 100]\n|    └─Linear: 2-11                      [3, 2]           [3, 50]          [2, 50]          150              300\n|    └─ReLU: 2-12                        [3, 50]          [3, 50]          --               --               --\n|    └─Linear: 2-13                      [3, 50]          [3, 100]         [50, 100]        5,100            15,000\n|    └─ReLU: 2-14                        [3, 100]         [3, 100]         --               --               --\n|    └─Linear: 2-15                      [3, 100]         [3, 784]         [100, 784]       79,184           235,200\n|    └─Sigmoid: 2-16                     [3, 784]         [3, 784]         --               --               --\n========================================================================================================================\nTotal params: 251,738\nTrainable params: 251,738\nNon-trainable params: 0\nTotal mult-adds (M): 2.00\n========================================================================================================================\nInput size (MB): 0.01\nForward/backward pass size (MB): 0.03\nParams size (MB): 1.01\nEstimated Total Size (MB): 1.05\n========================================================================================================================\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=de0be7a9-29e1-4ab6-9ce7-607fa646094e' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 2,
    "deepnote": {
      "is_reactive": false
    },
    "deepnote_notebook_id": "a45e7391-cf1c-46c6-9fab-a697156c90d2",
    "deepnote_execution_queue": []
  }
}