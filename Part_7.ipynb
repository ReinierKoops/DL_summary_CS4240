{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Part 7",
      "metadata": {
        "tags": [],
        "cell_id": "00000-f61d15d7-d0d8-4bb0-a0f8-2beea362b1f5",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Lecture 7\n\n**Word embeddings**: vector representation of a word, with semantic distances\n**Word2vec**: discrete word into a d-dimensional real valued feature vector.\n- Predict context\n    - Skip gram\n        - center word to k surrounding words\n    - Continuous bag-of-words (CBOW)\n        - surrounding k words to center word\n- **Cannot** use one-hot vectors to measure word similarities.\n\n<img src=\"./image/Predict_context.png\" height=\"150\" />\n",
      "metadata": {
        "tags": [],
        "cell_id": "00001-b0652b2a-a518-460c-91d5-4e24d844e12f",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Assignment 7: Self-attention and Transformers\nGiven a sequence of input vectors, the output of self-attention is a weighted sum of these vectors.\n\nSelf-attention is **permutation equivariant**: changing the order of the input sequence will result in the exact same output sequence, with also the same changes in the order. For a Transformer this could be detrimental. Ex. *Sequence classification* where the whole sequence is represented as a single feature vector. Append a *pooling layer* before classification layer, this would make the Transformer **permutation invariant**. As in it would give the same prediction regardless of the order of the sequence.\n\nThis can be alleviated by learning **positional embeddings**.\n- Make the term \"Deep Learning\" differ from \"Learning Deep\".\n\n<img src=\"./image/Three_operations.png\" height=\"300\" />\n\n**Self-attention**:\n- Learning how to re-weigh words with all other words.\n- No more recurrence; each vector can be computed in parallel.\n- Measure similarity between two input vectors\n- This is normalized between \\[0,1] (softmax) to act as probabilities\n- input tensor: \\[batch_size, sequence_length, embedding_dimension(dimension of input vector)]\n- Learn relationships between input vectors via three operations (implemented as linear layers *without bias*):\n    - **Query-mapping**: create Query (embedding_dimension x embedding_dimension)\n        - Calc: $$\\mathbf{q_i} = \\mathbf{W_q} \\mathbf{x_i}$$\n        - Ex: $$q_i$$ asks question to $$x_j$$ -> \"Are you a noun?\"\n    - **Key-mapping**: create Key (embedding_dimension x embedding_dimension)\n        - Calc: \n            - $$\\mathbf{k_i} = \\mathbf{W_k} \\mathbf{x_i}$$\n            - $$w_{ij}' = \\frac{\\mathbf{q_i}^T \\mathbf{k_j}}{\\sqrt{k}}$\n        - Ex: $$k_j$$ dot product with $$q_i$$ -> \"Noun-ness: high nr if true\"\n    - **Value-mapping**: create Value(embedding_dimension x embedding_dimension)\n        - Calc: \n            - $$\\mathbf{v_i} = \\mathbf{W_v} \\mathbf{x_i}$$\n            - $$w_{ij} = \\text{softmax}(w_{ij}')$$\n            - $$\\mathbf{y_i} = \\sum_j w_{ij}\\mathbf{v_j}$$\n        - Ex: if $$q_i^T$$ \\* $$k_j$$ is high, $$v_j$$ its value is mostly passed onto the output $$y_i$$\n- output tensor: \\[batch_size, sequence_length, embedding_dimension(dimension of input vector)]",
      "metadata": {
        "tags": [],
        "cell_id": "00001-ee43549e-f790-4e0e-803d-5e1b27fe548f",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00002-23a0b234-bb18-44c9-93b0-25bfa3d838f1",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "fc8e84bf",
        "execution_start": 1618040689326,
        "execution_millis": 6,
        "deepnote_cell_type": "code"
      },
      "source": "import torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torchinfo import summary\n\n\nclass SelfAttention(nn.Module):\n    \"\"\"\n    Self-attention operation with learnable key, query and value embeddings.\n    \n    Args:\n        k: embedding dimension\n        \n    \"\"\"\n    def __init__(self, k):\n        super(SelfAttention, self).__init__()\n\n        # These compute the queries, keys and values\n        self.tokeys    = nn.Linear(k, k, bias=False)\n        self.toqueries = nn.Linear(k, k, bias=False)\n        self.tovalues  = nn.Linear(k, k, bias=False)\n\n    def forward(self, x):\n\n        # Get tensor dimensions: batch size, sequence length and embedding dimension\n        b, t, k = x.size()\n\n        # Transform input tensor to queries, keys and values\n        queries = self.toqueries(x)\n        keys = self.tokeys(x)\n        values = self.tovalues(x)\n        \n        # Compute weights\n        w_prime = torch.bmm(queries, keys.transpose(1, 2))\n        # Scale weights\n        w_prime = w_prime / (k ** 0.5)\n        # Apply softmax\n        w = F.softmax(w_prime, dim=2)\n\n        # Compute output tensor\n        y = torch.bmm(w, values)\n\n        return y\n\n\n# Learnable parameters: 3 * embedding_dimension * embedding_dimension\nbatch_size, sequence_length, embedding_dimension = 4, 5, 6\nbias = False\n\nmodel_ouput = summary(\n    SelfAttention(embedding_dimension),\n    (batch_size, sequence_length, embedding_dimension),\n    verbose=2,\n    col_width=16,\n    col_names=[\"input_size\", \"output_size\", \"kernel_size\", \"num_params\", \"mult_adds\"],\n)",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "========================================================================================================================\nLayer (type:depth-idx)                   Input Shape      Output Shape     Kernel Shape     Param #          Mult-Adds\n========================================================================================================================\n├─Linear: 1-1                            [4, 5, 6]        [4, 5, 6]        [6, 6]           36               144\n├─Linear: 1-2                            [4, 5, 6]        [4, 5, 6]        [6, 6]           36               144\n├─Linear: 1-3                            [4, 5, 6]        [4, 5, 6]        [6, 6]           36               144\n========================================================================================================================\nTotal params: 108\nTrainable params: 108\nNon-trainable params: 0\nTotal mult-adds (M): 0.00\n========================================================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.00\nEstimated Total Size (MB): 0.00\n========================================================================================================================\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "**Multi-head (self-)attention**:\n- Adding multiple dimensions to the queries, keys and values vector, allows learning multiple relations (\"noun-ness\", \"verb-ness\", ...)\n- Two types:\n    - Narrow:\n        - Ex: map Embedding of dimension 256, with 8 attention heads to 8 different 32-dimensional query, key and value vectors. Then concatenated back to embedding dimension 256.\n        - **More computationally efficient**.\n        - Query, Key and values tensor: (embedding_dimension, embedding_dimension / heads)\n    - Wide:\n        - Ex: map Embedding of dimension 256, with 8 attenhion heads to 8 different 256-dimensional query, key and value vectors. then concatenated and mapped back to embedding dimension 256.\n        - **More expressive power**.\n        - Query, Key and values tensor: (embedding_dimension, embedding_dimension * heads)\n- input tensor: \\[batch_size * heads, sequence_length, embedding_dimension(dimension of input vector)]\n\n- output tensor: \\[batch_size, sequence_length, embedding_dimension(dimension of input vector)]\n",
      "metadata": {
        "tags": [],
        "cell_id": "00002-29c3181d-f8fb-4572-be2f-365e13a9701d",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00004-3423878b-e254-48cf-8f8d-36b70f51d80f",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "1954dfb3",
        "execution_millis": 8,
        "execution_start": 1618041700339,
        "deepnote_cell_type": "code"
      },
      "source": "import torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torchinfo import summary\n\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"\n    Wide mult-head self-attention layer.\n\n    Args:\n        k: embedding dimension\n        heads: number of heads (k mod heads must be 0)\n\n    \"\"\"\n    def __init__(self, k, heads=8):\n        super(MultiHeadAttention, self).__init__()\n\n        self.heads = heads\n\n        # These compute the queries, keys and values for all \n        # heads (as a single concatenated vector)\n        self.tokeys    = nn.Linear(k, k * heads, bias=False)\n        self.toqueries = nn.Linear(k, k * heads, bias=False)\n        self.tovalues  = nn.Linear(k, k * heads, bias=False)\n\n        # This unifies the outputs of the different heads into \n        # a single k-vector\n        self.unifyheads = nn.Linear(k * heads, k, bias=True)\n        \n    def forward(self, x):\n\n        b, t, k = x.size()\n        h = self.heads\n\n        # Project input to queries, keys and values\n        queries = self.toqueries(x).view(b, t, h, k)\n        keys    = self.tokeys(x).view(b, t, h, k)\n        values  = self.tovalues(x).view(b, t, h, k)\n\n        # Fold heads into the batch dimension\n        keys = keys.transpose(1, 2).reshape(b * h, t, k)\n        queries = queries.transpose(1, 2).reshape(b * h, t, k)\n        values = values.transpose(1, 2).reshape(b * h, t, k)\n        \n        # Compute attention weights\n        w_prime = torch.bmm(queries, keys.transpose(1, 2))\n        w_prime = w_prime / (k ** (1 / 2))\n        w = F.softmax(w_prime, dim=2)\n\n        # Apply the self-attention to the values\n        y = torch.bmm(w, values).view(b, h, t, k)\n\n        # Swap h, t back, unify heads\n        y = y.transpose(1, 2).reshape(b, t, h * k)\n\n        y = self.unifyheads(y)\n\n        return y\n\n## -- Multihead -- ##\n# -- Narrow -- Learnable parameters: \n#   ( 3 * embedding_dimension * ( embedding_dimension / heads ) )\n# + ( embedding_dimension / heads ) * embedding_dimension ( + embedding_dimension <- bias )\n\n# -- Wide -- Learnable parameters: \n#   ( 3 * embedding_dimension * ( embedding_dimension * heads ) )\n# + ( embedding_dimension * heads ) * embedding_dimension ( + embedding_dimension <- bias )\nbatch_size, sequence_length, embedding_dimension, heads = 4, 5, 6, 8\nbias = False\n\nmodel_ouput = summary(\n    MultiHeadAttention(embedding_dimension, heads),\n    (batch_size, sequence_length, embedding_dimension),\n    verbose=2,\n    col_width=16,\n    col_names=[\"input_size\", \"output_size\", \"kernel_size\", \"num_params\", \"mult_adds\"],\n)",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "========================================================================================================================\nLayer (type:depth-idx)                   Input Shape      Output Shape     Kernel Shape     Param #          Mult-Adds\n========================================================================================================================\n├─Linear: 1-1                            [4, 5, 6]        [4, 5, 48]       [6, 48]          288              1,152\n├─Linear: 1-2                            [4, 5, 6]        [4, 5, 48]       [6, 48]          288              1,152\n├─Linear: 1-3                            [4, 5, 6]        [4, 5, 48]       [6, 48]          288              1,152\n├─Linear: 1-4                            [4, 5, 48]       [4, 5, 6]        [48, 6]          294              1,152\n========================================================================================================================\nTotal params: 1,158\nTrainable params: 1,158\nNon-trainable params: 0\nTotal mult-adds (M): 0.00\n========================================================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.02\nParams size (MB): 0.00\nEstimated Total Size (MB): 0.03\n========================================================================================================================\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "**Transformer**:\n- Consist of self-attention layer, layer norm, Multilayer Perceptron and some residual connections.\n\n\n<img src=\"./image/Transformer.png\" height=\"300\" />",
      "metadata": {
        "tags": [],
        "cell_id": "00005-e7b24d7f-60b1-414e-99e1-70be4c4e2702",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=de0be7a9-29e1-4ab6-9ce7-607fa646094e' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 2,
    "deepnote": {
      "is_reactive": false
    },
    "deepnote_notebook_id": "b9f541fe-d3aa-45c0-a5dc-6b5c03caa30f",
    "deepnote_execution_queue": []
  }
}